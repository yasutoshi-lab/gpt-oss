# GPT-OSS Compact Model - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ»å®Ÿè¡Œã‚¬ã‚¤ãƒ‰

RTX 4090 16GBå‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸGPT-OSS Compactãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‹ã‚‰æ¨è«–ãƒ†ã‚¹ãƒˆã¾ã§ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶
- **GPU**: NVIDIA RTX 4090 16GBä»¥ä¸Šæ¨å¥¨
- **RAM**: 32GBä»¥ä¸Šæ¨å¥¨ï¼ˆ62GBä½¿ç”¨ç¢ºèªæ¸ˆã¿ï¼‰
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 10GBä»¥ä¸Šã®ç©ºãå®¹é‡
- **CPU**: Intel i9-14900HXç›¸å½“ä»¥ä¸Šæ¨å¥¨

### æ¤œè¨¼æ¸ˆã¿ç’°å¢ƒ
```bash
OS: Linux ubuntu 6.8.0-60-generic (Ubuntu 22.04)
Architecture: x86_64
GPU: NVIDIA GeForce RTX 4090 Laptop GPU (16376 MiB)
Python: 3.10.12
PyTorch: 2.6.0+cu124
CUDA: 12.4
uv: 0.8.4
```

## ğŸ“¦ äº‹å‰æº–å‚™

### 1. NVIDIA ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãƒ»CUDAã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# NVIDIA ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª
nvidia-smi

# CUDAãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### 2. uvãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# uvãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆ
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.cargo/env

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
uv --version
```

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### Step 1: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
```bash
# gpt-ossãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆã¾ãŸã¯æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ï¼‰
cd /path/to/gpt-oss
```

### Step 2: uvä»®æƒ³ç’°å¢ƒã®ä½œæˆ
```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆï¼ˆPython 3.10æŒ‡å®šï¼‰
uv venv --python 3.10 gpt-oss-compact

# ä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source gpt-oss-compact/bin/activate

# ç¢ºèª
which python
python --version
```

### Step 3: å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# PyTorch (CUDA 12.4å¯¾å¿œç‰ˆ)
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# è¿½åŠ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
uv pip install numpy

# GPUç¢ºèª
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"
```

### Step 4: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
```bash
# model_scratch.pyã®å­˜åœ¨ç¢ºèª
ls -la add_documents/model_scratch.py

# ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ç¢ºèª
chmod +x add_documents/model_scratch.py
```

## ğŸ”§ å®Ÿè¡Œæ‰‹é †

### 1. å­¦ç¿’ãƒ»æ¨è«–ã®å®Ÿè¡Œ
```bash
# GPT-OSS Compactãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ
cd /path/to/gpt-oss
python add_documents/model_scratch.py
```

### 2. æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›
```
=== GPT-OSS Compact Demo ===
RTX 4090 16GB optimized implementation
Using device: cuda
GPU Memory: 16.6GB
Model config: ModelConfig(num_hidden_layers=6, num_experts=8, ...)
Model created with 202,934,088 parameters (202.9M)
Estimated parameter memory: 0.4GB
Dataset created with 50 examples

Starting training demo...
Starting training for 1 epochs...
Model parameters: 202,934,088
Epoch 0, Step 0: Loss=10.4826, PPL=35690.48, Acc=0.000, LR=1.00e-04
Epoch 0, Step 10: Loss=10.0376, PPL=22870.22, Acc=0.000, LR=1.00e-04
Epoch 0 completed. Average Loss: 10.2871

Inference demo...
Prompt: 'Hello'
Generated: 'Hello...(ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ)'
Model saved to gpt_oss_compact.pt

Demo completed successfully!
```

### 3. ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
```bash
# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
ls -la gpt_oss_compact.pt
```

## ğŸ§ª æ¨è«–ãƒ†ã‚¹ãƒˆ

### ã‚«ã‚¹ã‚¿ãƒ æ¨è«–ã®å®Ÿè¡Œ
```python
#!/usr/bin/env python3
"""ã‚«ã‚¹ã‚¿ãƒ æ¨è«–ãƒ†ã‚¹ãƒˆ"""
import torch
from model_scratch import GPTOSSCompact, ModelConfig, SimpleTokenizer

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
config = ModelConfig()
model = GPTOSSCompact(config, device=device)
model.load_state_dict(torch.load('gpt_oss_compact.pt', map_location=device))
model.eval()

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
tokenizer = SimpleTokenizer()

# æ¨è«–ãƒ†ã‚¹ãƒˆ
def generate_text(prompt, max_length=50):
    tokens = tokenizer.encode(prompt)
    input_ids = torch.tensor([tokens], device=device)
    
    with torch.no_grad():
        logits, _ = model(input_ids)
        # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚°ãƒªãƒ¼ãƒ‡ã‚£ãƒ¼ç”Ÿæˆ
        for _ in range(max_length):
            next_token_logits = logits[0, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1)
            # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
            logits, _ = model(input_ids)
    
    return tokenizer.decode(input_ids[0].cpu().tolist())

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
result = generate_text("Hello world")
print(f"Generated: {result}")
```

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–
```python
# model_scratch.pyå†…ã®è¨­å®šèª¿æ•´ä¾‹

# ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ï¼ˆ100M parametersï¼‰
@dataclass
class ModelConfig:
    num_hidden_layers: int = 4      # 6 â†’ 4
    num_experts: int = 4           # 8 â†’ 4  
    experts_per_token: int = 2     # ç¶­æŒ
    hidden_size: int = 512         # 768 â†’ 512
    # ... ãã®ä»–è¨­å®š
```

### æ¨è«–é€Ÿåº¦ã®æœ€é©åŒ–
```python
# KV Cacheã‚’æ´»ç”¨ã—ãŸé«˜é€Ÿæ¨è«–
# model_scratch.pyå†…ã§æ—¢ã«å®Ÿè£…æ¸ˆã¿

# GPUæœ€é©åŒ–è¨­å®š
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ³•

#### 1. CUDA Out of Memory
```bash
# ã‚¨ãƒ©ãƒ¼: RuntimeError: CUDA out of memory
```
**è§£æ±ºæ³•**:
```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
batch_size = 2  # 4 â†’ 2

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
hidden_size = 512  # 768 â†’ 512
num_experts = 4    # 8 â†’ 4
```

#### 2. PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§
```bash
# ã‚¨ãƒ©ãƒ¼: BFloat16 not supported
```
**è§£æ±ºæ³•**:
```bash
# PyTorchã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
uv pip install torch==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124
```

#### 3. ä¾å­˜é–¢ä¿‚ã®å•é¡Œ
```bash
# ã‚¨ãƒ©ãƒ¼: Module not found
```
**è§£æ±ºæ³•**:
```bash
# ä»®æƒ³ç’°å¢ƒã®å†ä½œæˆ
uv venv --python 3.10 gpt-oss-compact-new
source gpt-oss-compact-new/bin/activate
uv pip install torch numpy
```

#### 4. GPUæ¤œå‡ºã®å•é¡Œ
```bash
# CUDA is not available
```
**è§£æ±ºæ³•**:
```bash
# CUDAãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª
nvidia-smi

# PyTorch CUDAç¢ºèª
python -c "import torch; print(torch.version.cuda)"

# å¿…è¦ã«å¿œã˜ã¦ãƒ‰ãƒ©ã‚¤ãƒãƒ¼å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```

## ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½æŒ‡æ¨™
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: ~203M
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ~0.4GB (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) + å­¦ç¿’ç”¨ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
- **å­¦ç¿’é€Ÿåº¦**: ~10ç§’/epoch (å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)
- **æ¨è«–é€Ÿåº¦**: ~100 tokens/sec
- **æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·**: 4096 tokens

### ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
```python
# GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
def print_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        print(f"GPU Memory - Allocated: {allocated:.1f}GB, Cached: {cached:.1f}GB")

print_gpu_memory()
```

## ğŸ”¬ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ»æ‹¡å¼µ

### ãƒ¢ãƒ‡ãƒ«æ§‹æˆã®å¤‰æ›´
```python
# add_documents/model_scratch.pyå†…ã®ModelConfig
@dataclass 
class ModelConfig:
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°èª¿æ•´
    num_hidden_layers: int = 8      # ã‚ˆã‚Šæ·±ã„ãƒ¢ãƒ‡ãƒ«
    
    # å°‚é–€å®¶æ•°èª¿æ•´  
    num_experts: int = 16           # ã‚ˆã‚Šå¤šãã®å°‚é–€å®¶
    experts_per_token: int = 4      # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–å°‚é–€å®¶æ•°
    
    # éš ã‚Œå±¤ã‚µã‚¤ã‚ºèª¿æ•´
    hidden_size: int = 1024         # ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›´
```python
# ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
class CustomDataset(Dataset):
    def __init__(self, texts: List[str], tokenizer, max_length=128):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __getitem__(self, idx):
        # ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…
        pass
```

## ğŸ“ å®Ÿè¡Œãƒ­ã‚°ä¾‹

### æˆåŠŸä¾‹
```
=== GPT-OSS Compact Demo ===
RTX 4090 16GB optimized implementation
Using device: cuda
GPU Memory: 16.6GB
Model config: ModelConfig(num_hidden_layers=6, num_experts=8, experts_per_token=2, vocab_size=32000, hidden_size=768, intermediate_size=1536, swiglu_limit=7.0, head_dim=64, num_attention_heads=12, num_key_value_heads=2, sliding_window=128, initial_context_length=1024, max_context_length=4096, rope_theta=10000.0, rope_scaling_factor=1.0, rope_ntk_alpha=1.0, rope_ntk_beta=32.0)
Model created with 202,934,088 parameters (202.9M)
Estimated parameter memory: 0.4GB
Dataset created with 50 examples

Starting training demo...
Starting training for 1 epochs...
Model parameters: 202,934,088
Epoch 0, Step 0: Loss=10.4826, PPL=35690.48, Acc=0.000, LR=1.00e-04
Epoch 0, Step 10: Loss=10.0376, PPL=22870.22, Acc=0.000, LR=1.00e-04
Epoch 0 completed. Average Loss: 10.2871

Inference demo...
Prompt: 'Hello'
Generated: 'Hello<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>c<UNK><UNK><UNK><UNK><UNK><UNK><UNK>'
Model saved to gpt_oss_compact.pt

Demo completed successfully!
Model saved as: gpt_oss_compact.pt
```

## ğŸ¤ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»ã‚µãƒãƒ¼ãƒˆ

### é–¢é€£ãƒªãƒ³ã‚¯
- [GPT-OSS GitHub Repository](https://github.com/gpt-oss/gpt-oss)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [uv Package Manager](https://github.com/astral-sh/uv)

### è²¢çŒ®ãƒ»æ”¹å–„
ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„ãƒ»æœ€é©åŒ–ã«é–¢ã™ã‚‹ææ¡ˆã‚„å•é¡Œå ±å‘Šã¯ã€GitHubãƒªãƒã‚¸ãƒˆãƒªã®Issuesã§ãŠé¡˜ã„ã—ã¾ã™ã€‚

---

**ä½œæˆæ—¥**: 2025-01-06  
**ãƒ†ã‚¹ãƒˆç’°å¢ƒ**: Ubuntu 22.04, RTX 4090 16GB, Python 3.10.12  
**ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: GPT-OSS Compact v1.0